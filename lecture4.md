class: middle, center, title-slide

# Deep Learning

Lecture 4: Training neural networks

<br><br>
Prof. Gilles Louppe<br>
[g.louppe@uliege.be](g.louppe@uliege.be)

???

- gradient descent: why minimizing the proxy is good (antoine)
- cs231b winter 2016 lecture 5 part 2 -> when does training fail, how to fix the activations
- include init, normalization, reg
- https://sif-dlv.github.io/slides/opt.pdf

R: have a coded example!!

https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture6_pauses.pdf

https://distill.pub/2017/momentum/

---

# Today

How to *optimize parameters* efficiently?

- Optimizers
- Regularization
- Initialization
- Normalization

---

class: middle

# Optimizers

---

# Batch gradient descent

xxx

---

illustration

---

# Stochastic gradient descent

xxx

---

## Learning rate schedule

---

## Mini-batching

---

illustration

---

# Momentum

---

# Adaptive learning rate

---

## AdaGrad

---

## RMSProp

---

## Adam

---

xxx check newer methods

---

class: middle

# Regularization

---

xxx

---

class: middle

# Initialization

---

xxx

---

class: middle

# Normalization

---

xxx

---

class: end-slide, center
count: false

The end.

---

# References

xxx
